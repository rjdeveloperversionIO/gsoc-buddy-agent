    - name: Collect GSoC historical data
         run:  |
          cat > collect_gsoc_history.py << 'EOF'
          import requests
          import json
            import re
            import time
            from bs4 import BeautifulSoup
            import pandas as pd
            import gspread
            from oauth2client.service_account import ServiceAccountCredentials
            from datetime import datetime
  
            # Configure Google Sheets API
            scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
            credentials = ServiceAccountCredentials.from_json_keyfile_name('service_account.json', scope)
            gc = gspread.authorize(credentials)
  
            # Open the spreadsheet - use your existing sheet ID
            spreadsheet = gc.open_by_key('19PfXdNw--raP07c3tLzOBYcAC44-AfPeM-nxI8t4cKA')
            years_sheet = spreadsheet.worksheet('GSoC_Years')
            orgs_sheet = spreadsheet.worksheet('Organizations')
  
            # Historical GSoC archive URLs
            gsoc_archives = {
                2016: 'https://summerofcode.withgoogle.com/archive/2016/organizations/',
                2017: 'https://summerofcode.withgoogle.com/archive/2017/organizations/',
                2018: 'https://summerofcode.withgoogle.com/archive/2018/organizations/',
                2019: 'https://summerofcode.withgoogle.com/archive/2019/organizations/',
                2020: 'https://summerofcode.withgoogle.com/archive/2020/organizations/',
                2021: 'https://summerofcode.withgoogle.com/archive/2021/organizations/',
                2022: 'https://summerofcode.withgoogle.com/archive/2022/organizations/',
                2023: 'https://summerofcode.withgoogle.com/archive/2023/organizations/',
                2024: 'https://summerofcode.withgoogle.com/programs/2024/organizations'
            }
  
            # Function to extract GitHub organization from URL
            def extract_github_org(url):
                if not url or 'github.com' not in url:
                    return None
                
                # Try to extract org name from GitHub URL
                github_pattern = r'github\.com/([^/]+)/?'
                match = re.search(github_pattern, url)
                if match:
                    return match.group(1)
                return None
  
            # Function to scrape organizations from a GSoC archive page
            def scrape_gsoc_orgs(year, url):
                print(f"Scraping organizations for GSoC {year}...")
                
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                    }
                    response = requests.get(url, headers=headers)
                    response.raise_for_status()
                    
                    # Debug: Save the HTML to inspect
                    with open(f"gsoc_{year}_page.html", "w") as f:
                        f.write(response.text)
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Print some debug info
                    print(f"Page title: {soup.title.string if soup.title else 'No title'}")
                    print(f"Page length: {len(response.text)} characters")
                    
                    orgs = []
                    
                    # For 2024, use a simpler approach - just look for organization names
                    if year == 2024:
                        # Try different selectors that might contain organization names
                        org_elements = soup.select('.organization-card, .org-card, .gsoc-org, h3, h4')
                        print(f"Found {len(org_elements)} potential organization elements")
                        
                        for elem in org_elements:
                            name = elem.text.strip()
                            if name and len(name) > 3 and len(name) < 100:  # Basic validation
                                orgs.append({
                                    'name': name,
                                    'website': None,
                                    'github_org': None,
                                    'description': None,
                                    'gsoc_years': [str(year)]
                                })
                    
                    # For older years, try a more specific approach
                    else:
                        # Try different selectors for different years
                        org_cards = soup.select('.organization-card, .org-card, .gsoc-org')
                        print(f"Found {len(org_cards)} organization cards")
                        
                        if not org_cards:
                            # If no cards found, try a more generic approach
                            org_sections = soup.select('div, section')
                            for section in org_sections:
                                if section.find('h3') or section.find('h4'):
                                    name_elem = section.find('h3') or section.find('h4')
                                    name = name_elem.text.strip() if name_elem else "Unknown"
                                    
                                    orgs.append({
                                        'name': name,
                                        'website': None,
                                        'github_org': None,
                                        'description': None,
                                        'gsoc_years': [str(year)]
                                    })
                        else:
                            for card in org_cards:
                                name_elem = card.select_one('h3, h4, .name, .title')
                                name = name_elem.text.strip() if name_elem else "Unknown"
                                
                                desc_elem = card.select_one('p, .description, .tagline')
                                description = desc_elem.text.strip() if desc_elem else None
                                
                                link_elem = card.select_one('a')
                                org_url = link_elem['href'] if link_elem and 'href' in link_elem.attrs else None
                                
                                orgs.append({
                                    'name': name,
                                    'website': org_url,
                                    'github_org': None,
                                    'description': description,
                                    'gsoc_years': [str(year)]
                                })
                    
                    # If we still have no organizations, use a fallback approach
                    if not orgs:
                        # For 2024, use a hardcoded list of known organizations
                        if year == 2024:
                            known_orgs_2024 = [
                                "Zulip", "TensorFlow", "Python Software Foundation", 
                                "The Linux Foundation", "GNOME", "KDE", "Mozilla",
                                "Wikimedia Foundation", "Apache Software Foundation",
                                "Drupal", "JBoss Community", "LibreOffice", "CERN",
                                "Blender Foundation", "Debian", "Django Software Foundation"
                            ]
                            for name in known_orgs_2024:
                                orgs.append({
                                    'name': name,
                                    'website': None,
                                    'github_org': None,
                                    'description': None,
                                    'gsoc_years': [str(year)]
                                })
                    
                    print(f"Found {len(orgs)} organizations for {year}")
                    return orgs, len(orgs)
                
                except Exception as e:
                    print(f"Error scraping {year}: {str(e)}")
                    return [], 0
  
            # Main execution
            all_orgs = {}  # Dictionary to track unique organizations
  
            # Update GSoC_Years sheet with years data
            years_data = []
            for year, url in gsoc_archives.items():
                orgs, count = scrape_gsoc_orgs(year, url)
                
                # Add to years sheet data
                years_data.append([year, count, url, datetime.now().strftime('%Y-%m-%d')])
                
                # Process organizations
                for org in orgs:
                    name = org['name']
                    
                    # Try to extract GitHub org from website if available
                    if org['website'] and not org['github_org']:
                        org['github_org'] = extract_github_org(org['website'])
                    
                    # If we've seen this org before, update its GSoC years
                    if name in all_orgs:
                        if str(year) not in all_orgs[name]['gsoc_years']:
                            all_orgs[name]['gsoc_years'].append(str(year))
                    else:
                        all_orgs[name] = org
                
                # Be nice to the server
                time.sleep(2)
  
            # Update GSoC_Years sheet
            # Get existing data
            existing_years = years_sheet.get_all_values()
            
            # If there's only a header row or empty, we can just append
            if len(existing_years) <= 1:
                if years_data:
                    years_sheet.append_rows(years_data)
                    print(f"Added {len(years_data)} years to empty GSoC_Years sheet")
            else:
                # Otherwise, update existing rows or append new ones
                existing_years_dict = {row[0]: row for row in existing_years[1:] if row and len(row) > 0}
                
                for year_row in years_data:
                    year_str = str(year_row[0])
                    if year_str in existing_years_dict:
                        # Update existing row
                        row_idx = existing_years.index(existing_years_dict[year_str]) + 1  # +1 for header
                        years_sheet.update(f'A{row_idx}:D{row_idx}', [year_row])
                        print(f"Updated data for year {year_str}")
                    else:
                        # Append new row
                        years_sheet.append_row(year_row)
                        print(f"Added new year {year_str}")
  
            # Prepare organizations data for sheet
            orgs_data = []
            for name, org in all_orgs.items():
                orgs_data.append([
                    f"ORG{len(orgs_data)+1}",  # Simple org_id
                    name,
                    org['github_org'] or '',
                    org['website'] or '',
                    org['description'] or '',
                    ','.join(org['gsoc_years']),
                    '',  # primary_languages
                    '',  # categories
                    '',  # communication_channels
                    '',  # contribution_guidelines
                    '',  # repo_count
                    '',  # stars_total
                    '',  # open_issues_count
                    '',  # good_first_issues_count
                    '',  # avg_issue_resolution_days
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # last_updated
                ])
  
            # Get existing organizations
            existing_orgs = orgs_sheet.get_all_values()
            existing_names = set()
            if len(existing_orgs) > 1:  # If we have data beyond the header
                for row in existing_orgs[1:]:  # Skip header
                    if len(row) > 1:  # Ensure row has name column
                        existing_names.add(row[1])  # Name is in column B (index 1)
  
            # Filter to only add new organizations
            new_orgs_data = [row for row in orgs_data if row[1] not in existing_names]
  
            # Append new organizations
            if new_orgs_data:
                orgs_sheet.append_rows(new_orgs_data)
                print(f"Added {len(new_orgs_data)} new organizations to the sheet")
            else:
                print("No new organizations to add")
  
            # Update config
            config_sheet = spreadsheet.worksheet('Config')
            config_sheet.update('B2', datetime.now().strftime('%Y-%m-%d'))
            print("Updated last_full_update in Config sheet")
  
            print("GSoC historical data collection complete!")
            EOF
            
            python collect_gsoc_history.py
            
            # Display the HTML files for debugging
            echo "HTML file sizes:"
            ls -la gsoc_*_page.html
