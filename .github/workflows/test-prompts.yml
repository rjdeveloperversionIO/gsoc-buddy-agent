name: Test Prompt Templates

on:
  workflow_dispatch:  # Allows manual triggering

jobs:
  test-prompts:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
      
      - name: Test prompt loader
        run: |
          python scripts/prompt_loader.py
      
      - name: Test prompts with AI
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          cat > test_prompts.py << 'EOF'
          import sys
          import json
          from scripts.prompt_loader import get_prompt, list_available_prompts
          from scripts.ai_integration import get_ai_response, get_json_response
          
          def test_issue_analysis_prompt():
              """Test the issue analysis prompt"""
              print("\n--- Testing Issue Analysis Prompt ---")
              
              # Test variables
              test_variables = {
                  'title': 'Fix button alignment in dark mode',
                  'repo_full_name': 'example/repo',
                  'org_name': 'Example Organization',
                  'body_excerpt': 'The submit button is misaligned when using dark mode. It appears 10px to the right of where it should be.',
                  'labels': ['bug', 'good first issue', 'css', 'ui']
              }
              
              # Get the prompt
              prompt = get_prompt('issue_analysis', 'basic_analysis', test_variables)
              if not prompt:
                  print("Failed to load issue analysis prompt")
                  return False
              
              # Get AI response
              try:
                  response = get_json_response(prompt, temperature=0.1)
                  print("AI Response:")
                  print(json.dumps(response, indent=2))
                  
                  # Verify required fields
                  required_fields = ['difficulty_score', 'required_skills', 'estimated_time_hours', 'beginner_friendly', 'rationale']
                  missing_fields = [field for field in required_fields if field not in response]
                  
                  if missing_fields:
                      print(f"Warning: Response is missing required fields: {', '.join(missing_fields)}")
                      return False
                  
                  print("Issue analysis prompt test: PASSED")
                  return True
              except Exception as e:
                  print(f"Error testing issue analysis prompt: {str(e)}")
                  return False
          
          def test_resolution_guide_prompt():
              """Test the resolution guide prompt"""
              print("\n--- Testing Resolution Guide Prompt ---")
              
              # Test variables
              test_variables = {
                  'title': 'Fix button alignment in dark mode',
                  'repo_full_name': 'example/repo',
                  'org_name': 'Example Organization',
                  'body_excerpt': 'The submit button is misaligned when using dark mode. It appears 10px to the right of where it should be.',
                  'labels': ['bug', 'good first issue', 'css', 'ui'],
                  'required_skills': ['CSS', 'HTML', 'UI Design', 'Browser DevTools'],
                  'difficulty_score': 2,
                  'student_experience': 'Beginner with basic HTML/CSS knowledge'
              }
              
              # Get the prompt
              prompt = get_prompt('guide_generation', 'resolution_guide', test_variables)
              if not prompt:
                  print("Failed to load resolution guide prompt")
                  return False
              
              # Get AI response
              try:
                  response = get_ai_response(prompt, temperature=0.3)
                  print("AI Response (truncated):")
                  print(response[:300] + "...\n[Response truncated for brevity]")
                  
                  # Check for expected sections
                  expected_sections = ['Understanding the Issue', 'Environment Setup', 'Step-by-Step Approach', 'Testing']
                  missing_sections = [section for section in expected_sections if section not in response]
                  
                  if missing_sections:
                      print(f"Warning: Response is missing expected sections: {', '.join(missing_sections)}")
                      return False
                  
                  print("Resolution guide prompt test: PASSED")
                  return True
              except Exception as e:
                  print(f"Error testing resolution guide prompt: {str(e)}")
                  return False
          
          # Main execution
          print("Testing prompt templates...")
          
          # List available prompts
          available_prompts = list_available_prompts()
          print("Available prompts:")
          for prompt_type, prompt_names in available_prompts.items():
              print(f"  {prompt_type}: {', '.join(prompt_names)}")
          
          # Run tests
          tests = [
              test_issue_analysis_prompt,
              test_resolution_guide_prompt
          ]
          
          results = [test() for test in tests]
          
          # Print summary
          print("\n--- Test Summary ---")
          print(f"Tests passed: {results.count(True)}/{len(tests)}")
          
          # Exit with appropriate code
          sys.exit(0 if all(results) else 1)
          EOF
          
          python test_prompts.py
